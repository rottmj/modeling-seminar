{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-14 23:03:14.724058: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-07-14 23:03:14.921096: E tensorflow/stream_executor/cuda/cuda_blas.cc:2982] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-07-14 23:03:15.747829: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-07-14 23:03:15.747987: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-07-14 23:03:15.748002: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from DDPG import ddpg\n",
    "from environment.vehicleEnv import VehicleEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# vehicles\n",
    "LEAD_VELOCITY = 27.78\n",
    "LEAD_POSITION = 100.0\n",
    "INITIAL_EGO_VELOCITY = 15.0\n",
    "EGO_POSITION = 0.0\n",
    "SET_EGO_VELOCITY = 30\n",
    "\n",
    "SAFETY_DISTANCE = 50\n",
    "# INITIAL_GAP = 100\n",
    "\n",
    "# network\n",
    "GAMMA = 0.99\n",
    "BATCH_SIZE = 64\n",
    "MEMORY_SIZE = 500000\n",
    "LEARNING_RATE_ACTOR = 0.001\n",
    "LEARNING_RATE_CRITIC = 0.0001\n",
    "TAU = 0.001\n",
    "NOISE_MEAN = 0\n",
    "NOISE_SIGMA = 0.02\n",
    "\n",
    "INPUT_SHAPE = (2,)\n",
    "\n",
    "# simulation\n",
    "N_SIMULATIONS = 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-14 23:03:16.538371: E tensorflow/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error\n",
      "2022-07-14 23:03:16.538401: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: bobby-OMEN-by-HP-Laptop-15-dh1xxx\n",
      "2022-07-14 23:03:16.538408: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: bobby-OMEN-by-HP-Laptop-15-dh1xxx\n",
      "2022-07-14 23:03:16.538468: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 515.48.7\n",
      "2022-07-14 23:03:16.538489: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 515.48.7\n",
      "2022-07-14 23:03:16.538494: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 515.48.7\n",
      "2022-07-14 23:03:16.538759: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# initialize networks\n",
    "agent = ddpg.Agent(\n",
    "            input_shape=INPUT_SHAPE, \n",
    "            alpha=LEARNING_RATE_ACTOR, \n",
    "            beta=LEARNING_RATE_CRITIC, \n",
    "            gamma=GAMMA, \n",
    "            n_actions=1, \n",
    "            max_size=MEMORY_SIZE, \n",
    "            tau=TAU, \n",
    "            batch_size=BATCH_SIZE, \n",
    "            noise=NOISE_SIGMA\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_simulation():\n",
    "    state_list = []\n",
    "    speed_ego_list = []\n",
    "    speed_lead_list = []\n",
    "    action_list = []\n",
    "    reward_list = []\n",
    "    \n",
    "\n",
    "    env = VehicleEnv(\n",
    "            lead_velocity=LEAD_VELOCITY, \n",
    "            lead_position=LEAD_POSITION, \n",
    "            ego_velocity=INITIAL_EGO_VELOCITY, \n",
    "            ego_position=EGO_POSITION)\n",
    "    state = env.get_state()\n",
    "    done = False\n",
    "    return_episode = 0\n",
    "    steps = 0\n",
    "    for i in range(200):\n",
    "        #print(\"State: \", state)\n",
    "        state_list.append(state)\n",
    "        #print(\"Speed ego: \", env.ego.get_velocity())\n",
    "        speed_ego_list.append(env.ego.get_velocity()[0])\n",
    "        #print(\"Speed lead: \", env.lead.get_velocity())\n",
    "        speed_lead_list.append(env.lead.get_velocity()[0])\n",
    "        # Select an action\n",
    "        action = agent.choose_action(state)\n",
    "         \n",
    "        # Perform the action\n",
    "        next_state, reward, done = env.step(action)\n",
    "        \n",
    "        assert type(env.ego.get_velocity()) != int\n",
    "\n",
    "       \n",
    "        #print(\"Action: \", action)\n",
    "        action_list.append(action)\n",
    "        #print(\"Next state: \", next_state)\n",
    "        #print(\"Reward: \", reward)\n",
    "        reward_list.append(reward)\n",
    "        #print(\"Done: \", done)\n",
    "        \n",
    "        # Store the transition\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "                \n",
    "        # Training\n",
    "        agent.learn()\n",
    "                \n",
    "        # Go in the next state\n",
    "        state = next_state\n",
    "                \n",
    "        # Increment time\n",
    "        # steps += 1\n",
    "        return_episode += reward\n",
    "        steps += 1\n",
    "        if done:\n",
    "            break\n",
    "    return return_episode, steps, reward_list# , state_list, speed_ego_list, speed_lead_list, action_list,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "return_episode = []\n",
    "steps_episode = []\n",
    "reward_episode = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "steps:  200\n",
      "14.9529 seconds\n",
      "1\n",
      "steps:  400\n",
      "20.1009 seconds\n",
      "2\n",
      "steps:  600\n",
      "20.4736 seconds\n",
      "3\n",
      "steps:  800\n",
      "20.7154 seconds\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/bobby/Modellierungsseminar/Code/learnACC.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/bobby/Modellierungsseminar/Code/learnACC.ipynb#ch0000008?line=3'>4</a>\u001b[0m \u001b[39mwhile\u001b[39;00m num \u001b[39m<\u001b[39m \u001b[39m1000000\u001b[39m:\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/bobby/Modellierungsseminar/Code/learnACC.ipynb#ch0000008?line=4'>5</a>\u001b[0m     tic \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mperf_counter()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/bobby/Modellierungsseminar/Code/learnACC.ipynb#ch0000008?line=5'>6</a>\u001b[0m     return_episod, steps, reward_list \u001b[39m=\u001b[39m one_simulation()\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/bobby/Modellierungsseminar/Code/learnACC.ipynb#ch0000008?line=6'>7</a>\u001b[0m     return_episode\u001b[39m.\u001b[39mappend(return_episod)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/bobby/Modellierungsseminar/Code/learnACC.ipynb#ch0000008?line=7'>8</a>\u001b[0m     steps_episode\u001b[39m.\u001b[39mappend(steps)\n",
      "\u001b[1;32m/home/bobby/Modellierungsseminar/Code/learnACC.ipynb Cell 9\u001b[0m in \u001b[0;36mone_simulation\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bobby/Modellierungsseminar/Code/learnACC.ipynb#ch0000008?line=41'>42</a>\u001b[0m agent\u001b[39m.\u001b[39mremember(state, action, reward, next_state, done)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bobby/Modellierungsseminar/Code/learnACC.ipynb#ch0000008?line=43'>44</a>\u001b[0m \u001b[39m# Training\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/bobby/Modellierungsseminar/Code/learnACC.ipynb#ch0000008?line=44'>45</a>\u001b[0m agent\u001b[39m.\u001b[39;49mlearn()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bobby/Modellierungsseminar/Code/learnACC.ipynb#ch0000008?line=46'>47</a>\u001b[0m \u001b[39m# Go in the next state\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bobby/Modellierungsseminar/Code/learnACC.ipynb#ch0000008?line=47'>48</a>\u001b[0m state \u001b[39m=\u001b[39m next_state\n",
      "File \u001b[0;32m~/Modellierungsseminar/Code/DDPG/ddpg.py:131\u001b[0m, in \u001b[0;36mAgent.learn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    127\u001b[0m     critic_loss \u001b[39m=\u001b[39m keras\u001b[39m.\u001b[39mlosses\u001b[39m.\u001b[39mMSE(target, critic_value)\n\u001b[1;32m    129\u001b[0m critic_network_gradient \u001b[39m=\u001b[39m tape\u001b[39m.\u001b[39mgradient(critic_loss,\n\u001b[1;32m    130\u001b[0m                                         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcritic\u001b[39m.\u001b[39mtrainable_variables)\n\u001b[0;32m--> 131\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcritic\u001b[39m.\u001b[39;49moptimizer\u001b[39m.\u001b[39;49mapply_gradients(\u001b[39mzip\u001b[39;49m(\n\u001b[1;32m    132\u001b[0m     critic_network_gradient, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcritic\u001b[39m.\u001b[39;49mtrainable_variables))\n\u001b[1;32m    134\u001b[0m \u001b[39m# Update actor\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mGradientTape() \u001b[39mas\u001b[39;00m tape:\n",
      "File \u001b[0;32m~/miniconda3/envs/sumo/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/optimizer_v2.py:739\u001b[0m, in \u001b[0;36mOptimizerV2.apply_gradients\u001b[0;34m(self, grads_and_vars, name, experimental_aggregate_gradients)\u001b[0m\n\u001b[1;32m    735\u001b[0m     grads_and_vars \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_transform_unaggregated_gradients(\n\u001b[1;32m    736\u001b[0m         grads_and_vars\n\u001b[1;32m    737\u001b[0m     )\n\u001b[1;32m    738\u001b[0m     grads_and_vars \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_aggregate_gradients(grads_and_vars)\n\u001b[0;32m--> 739\u001b[0m grads_and_vars \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_transform_gradients(grads_and_vars)\n\u001b[1;32m    741\u001b[0m \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39m__internal__\u001b[39m.\u001b[39mdistribute\u001b[39m.\u001b[39minterim\u001b[39m.\u001b[39mmaybe_merge_call(\n\u001b[1;32m    742\u001b[0m     functools\u001b[39m.\u001b[39mpartial(\n\u001b[1;32m    743\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_distributed_apply, apply_state\u001b[39m=\u001b[39mapply_state\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    747\u001b[0m     name\u001b[39m=\u001b[39mname,\n\u001b[1;32m    748\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/sumo/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/optimizer_v2.py:540\u001b[0m, in \u001b[0;36mOptimizerV2._transform_gradients\u001b[0;34m(self, grads_and_vars)\u001b[0m\n\u001b[1;32m    538\u001b[0m     grads_and_vars \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_clipvalue_fn(grads_and_vars)\n\u001b[1;32m    539\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_clipnorm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 540\u001b[0m     grads_and_vars \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_clipnorm_fn(grads_and_vars)\n\u001b[1;32m    541\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_global_clipnorm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    542\u001b[0m     grads_and_vars \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_global_clipnorm_fn(grads_and_vars)\n",
      "File \u001b[0;32m~/miniconda3/envs/sumo/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/utils.py:112\u001b[0m, in \u001b[0;36mmake_gradient_clipnorm_fn.<locals>.gradient_clipnorm_fn\u001b[0;34m(grads_and_vars)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\n\u001b[1;32m    101\u001b[0m     tf\u001b[39m.\u001b[39mdistribute\u001b[39m.\u001b[39mget_strategy(),\n\u001b[1;32m    102\u001b[0m     (\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    105\u001b[0m     ),\n\u001b[1;32m    106\u001b[0m ):\n\u001b[1;32m    107\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    108\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`clipnorm` is not supported with `CenteralStorageStrategy`. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    109\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe strategy used is \u001b[39m\u001b[39m{\u001b[39;00mtf\u001b[39m.\u001b[39mdistribute\u001b[39m.\u001b[39mget_strategy()\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    110\u001b[0m     )\n\u001b[0;32m--> 112\u001b[0m clipped_grads_and_vars \u001b[39m=\u001b[39m [\n\u001b[1;32m    113\u001b[0m     (tf\u001b[39m.\u001b[39mclip_by_norm(g, clipnorm), v) \u001b[39mfor\u001b[39;00m g, v \u001b[39min\u001b[39;00m grads_and_vars\n\u001b[1;32m    114\u001b[0m ]\n\u001b[1;32m    115\u001b[0m \u001b[39mreturn\u001b[39;00m clipped_grads_and_vars\n",
      "File \u001b[0;32m~/miniconda3/envs/sumo/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/utils.py:113\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\n\u001b[1;32m    101\u001b[0m     tf\u001b[39m.\u001b[39mdistribute\u001b[39m.\u001b[39mget_strategy(),\n\u001b[1;32m    102\u001b[0m     (\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    105\u001b[0m     ),\n\u001b[1;32m    106\u001b[0m ):\n\u001b[1;32m    107\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    108\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`clipnorm` is not supported with `CenteralStorageStrategy`. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    109\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe strategy used is \u001b[39m\u001b[39m{\u001b[39;00mtf\u001b[39m.\u001b[39mdistribute\u001b[39m.\u001b[39mget_strategy()\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    110\u001b[0m     )\n\u001b[1;32m    112\u001b[0m clipped_grads_and_vars \u001b[39m=\u001b[39m [\n\u001b[0;32m--> 113\u001b[0m     (tf\u001b[39m.\u001b[39;49mclip_by_norm(g, clipnorm), v) \u001b[39mfor\u001b[39;00m g, v \u001b[39min\u001b[39;00m grads_and_vars\n\u001b[1;32m    114\u001b[0m ]\n\u001b[1;32m    115\u001b[0m \u001b[39mreturn\u001b[39;00m clipped_grads_and_vars\n",
      "File \u001b[0;32m~/miniconda3/envs/sumo/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/sumo/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:1176\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1174\u001b[0m \u001b[39m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[1;32m   1175\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1176\u001b[0m   \u001b[39mreturn\u001b[39;00m dispatch_target(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1177\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[1;32m   1178\u001b[0m   \u001b[39m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[1;32m   1179\u001b[0m   \u001b[39m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[1;32m   1180\u001b[0m   result \u001b[39m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/sumo/lib/python3.9/site-packages/tensorflow/python/ops/clip_ops.py:222\u001b[0m, in \u001b[0;36mclip_by_norm\u001b[0;34m(t, clip_norm, axes, name)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[39m# Two-tap tf.where trick to bypass NaN gradients\u001b[39;00m\n\u001b[1;32m    221\u001b[0m l2sum_safe \u001b[39m=\u001b[39m array_ops\u001b[39m.\u001b[39mwhere(pred, l2sum, array_ops\u001b[39m.\u001b[39mones_like(l2sum))\n\u001b[0;32m--> 222\u001b[0m l2norm \u001b[39m=\u001b[39m array_ops\u001b[39m.\u001b[39;49mwhere(pred, math_ops\u001b[39m.\u001b[39;49msqrt(l2sum_safe), l2sum)\n\u001b[1;32m    223\u001b[0m intermediate \u001b[39m=\u001b[39m values \u001b[39m*\u001b[39m clip_norm\n\u001b[1;32m    224\u001b[0m \u001b[39m# Assert that the shape is compatible with the initial shape,\u001b[39;00m\n\u001b[1;32m    225\u001b[0m \u001b[39m# to prevent unintentional broadcasting.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/sumo/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/sumo/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:1176\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1174\u001b[0m \u001b[39m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[1;32m   1175\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1176\u001b[0m   \u001b[39mreturn\u001b[39;00m dispatch_target(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1177\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[1;32m   1178\u001b[0m   \u001b[39m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[1;32m   1179\u001b[0m   \u001b[39m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[1;32m   1180\u001b[0m   result \u001b[39m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/sumo/lib/python3.9/site-packages/tensorflow/python/ops/array_ops.py:4730\u001b[0m, in \u001b[0;36mwhere\u001b[0;34m(condition, x, y, name)\u001b[0m\n\u001b[1;32m   4728\u001b[0m     \u001b[39mreturn\u001b[39;00m gen_array_ops\u001b[39m.\u001b[39mwhere(condition\u001b[39m=\u001b[39mcondition, name\u001b[39m=\u001b[39mname)\n\u001b[1;32m   4729\u001b[0m \u001b[39melif\u001b[39;00m x \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 4730\u001b[0m   \u001b[39mreturn\u001b[39;00m gen_math_ops\u001b[39m.\u001b[39;49mselect(condition\u001b[39m=\u001b[39;49mcondition, x\u001b[39m=\u001b[39;49mx, y\u001b[39m=\u001b[39;49my, name\u001b[39m=\u001b[39;49mname)\n\u001b[1;32m   4731\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   4732\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mx and y must both be non-None or both be None.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/sumo/lib/python3.9/site-packages/tensorflow/python/ops/gen_math_ops.py:9341\u001b[0m, in \u001b[0;36mselect\u001b[0;34m(condition, x, y, name)\u001b[0m\n\u001b[1;32m   9339\u001b[0m \u001b[39mif\u001b[39;00m tld\u001b[39m.\u001b[39mis_eager:\n\u001b[1;32m   9340\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 9341\u001b[0m     _result \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_FastPathExecute(\n\u001b[1;32m   9342\u001b[0m       _ctx, \u001b[39m\"\u001b[39;49m\u001b[39mSelect\u001b[39;49m\u001b[39m\"\u001b[39;49m, name, condition, x, y)\n\u001b[1;32m   9343\u001b[0m     \u001b[39mreturn\u001b[39;00m _result\n\u001b[1;32m   9344\u001b[0m   \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "num = 0\n",
    "i = 0\n",
    "while num < 1000000:\n",
    "    tic = time.perf_counter()\n",
    "    return_episod, steps, reward_list = one_simulation()\n",
    "    return_episode.append(return_episod)\n",
    "    steps_episode.append(steps)\n",
    "    reward_episode.append(reward_list)\n",
    "    print(i)\n",
    "    i+=1\n",
    "    num+=steps\n",
    "    print(\"steps: \", num)\n",
    "    tac = time.perf_counter()\n",
    "    print(f\"{tac-tic:0.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_simulation():\n",
    "    state_list = []\n",
    "    speed_ego_list = []\n",
    "    speed_lead_list = []\n",
    "    action_list = []\n",
    "    reward_list = []\n",
    "    \n",
    "\n",
    "    env = VehicleEnv(\n",
    "            lead_velocity=LEAD_VELOCITY, \n",
    "            lead_position=LEAD_POSITION, \n",
    "            ego_velocity=INITIAL_EGO_VELOCITY, \n",
    "            ego_position=EGO_POSITION)\n",
    "    state = env.get_state()\n",
    "    done = False\n",
    "    return_episode = 0\n",
    "    steps = 0\n",
    "    for i in range(200):\n",
    "        #print(\"State: \", state)\n",
    "        state_list.append(state)\n",
    "        #print(\"Speed ego: \", env.ego.get_velocity())\n",
    "        speed_ego_list.append(env.ego.get_velocity()[0])\n",
    "        #print(\"Speed lead: \", env.lead.get_velocity())\n",
    "        speed_lead_list.append(env.lead.get_velocity()[0])\n",
    "        # Select an action\n",
    "        action = agent.choose_action(state)\n",
    "         \n",
    "        # Perform the action\n",
    "        next_state, reward, done = env.step(action)\n",
    "        \n",
    "        assert type(env.ego.get_velocity()) != int\n",
    "\n",
    "       \n",
    "        #print(\"Action: \", action)\n",
    "        action_list.append(action)\n",
    "        #print(\"Next state: \", next_state)\n",
    "        #print(\"Reward: \", reward)\n",
    "        reward_list.append(reward)\n",
    "        #print(\"Done: \", done)\n",
    "        \n",
    "        # Store the transition\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "                \n",
    "        # Training\n",
    "        agent.learn()\n",
    "                \n",
    "        # Go in the next state\n",
    "        state = next_state\n",
    "                \n",
    "        # Increment time\n",
    "        # steps += 1\n",
    "        return_episode += reward\n",
    "        steps += 1\n",
    "        if done:\n",
    "            break\n",
    "    return state_list, speed_ego_list, speed_lead_list, action_list\n",
    "\n",
    "state_list, speed_ego_list, speed_lead_list, action_list = test_simulation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_reward(reward_list, path=\"graphs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_control_input(action_list, \"graphs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_speed(speed_ego_list, \"graphs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_undiscounted_episode_reward(return_episode, \"graphs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('rewrds.txt', 'wb') as fh:\n",
    "   pickle.dump(reward_episode, fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle_off = open (\"steps.txt\", \"rb\")\n",
    "emp = pickle.load(pickle_off)\n",
    "print(emp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.save_models()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('sumo')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e08b9c5f9e065d04fc6914c6138f7802456f17ba40be99f1125c507ccac176fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
